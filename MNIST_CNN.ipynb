{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOPe+/fE4DD/0RI2Yefgn7f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunSabnis/pytorch_course_udemy/blob/master/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIA_JWMqzFoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx8TVMqN9mv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnSoERkotFwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KivfuO5k97_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize((28,28)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))]) # mean and standard deviation of 0.5 for channel 1, here image has only one channel\n",
        "training_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "validation_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=100, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFd0fdk0R4WI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_convert(im_tensor):\n",
        "  image = im_tensor.cpu().clone().detach().numpy() \n",
        "  image = image.transpose(1, 2, 0) # Change shape of image from 1*28*28 to 28*28*1\n",
        "  image = image*(np.array((0.5, 0.5, 0.5))) + np.array((0.5, 0.5, 0.5)) \n",
        "  \"\"\"\n",
        "  y = (x - mean)/std\n",
        "  x = y*std + mean  -- this will get back original image that was normalized\n",
        "  \"\"\"\n",
        "  image = image.clip(0, 1) # Keep every pixel between 0 and 1\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpmGCl-RQhh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(training_loader)\n",
        "images, labels = dataiter.next()\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for i in np.arange(20):\n",
        "  ax = fig.add_subplot(2, 10, i+1)\n",
        "  plt.imshow(image_convert(images[i]))\n",
        "  ax.set_title(labels[i].item())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wk9TRskcIvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "  def compute_out_size(self, num_prev_layer_features, num_filters, kernel_size, stride=1, num_prev_layer_channels=1):\n",
        "    return(int(((num_prev_layer_features - kernel_size)/stride + 1)/2), num_filters)\n",
        "\n",
        "  def __init__(self, img_dim, filters, kernel_sizes, fc_layers, im_channels=1, pool_size=2, stride=1):\n",
        "    self.pool_size = pool_size\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(im_channels, filters[0], kernel_sizes[0], 1)\n",
        "    conv1_out_size, _ = self.compute_out_size(img_dim, filters[0], kernel_sizes[0])\n",
        "    self.conv2 = nn.Conv2d(filters[0], filters[1], kernel_sizes[1], 1)\n",
        "    conv2_out_size, _ = self.compute_out_size(conv1_out_size, filters[1], kernel_sizes[1])\n",
        "    self.fc_layer_input = conv2_out_size*conv2_out_size*filters[1]\n",
        "    self.fc1 = nn.Linear(conv2_out_size*conv2_out_size*filters[1], fc_layers[0])\n",
        "    self.dropout1 = nn.Dropout(0.5)\n",
        "    self.fc2 = nn.Linear(fc_layers[0], fc_layers[1])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, self.pool_size, self.pool_size)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, self.pool_size, self.pool_size)\n",
        "    x = x.view(-1, self.fc_layer_input)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpVNBjK1oGdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_shape(images):\n",
        "  image = image_convert(images[0])\n",
        "  return image.shape[0], image.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAjOQngLlkie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_shape_x, im_shape_y = get_image_shape(images)\n",
        "# For nxn type images and kernels with no extra  padding  \n",
        "model = LeNet(im_shape_x, filters=[20, 50], kernel_sizes=[5,5], fc_layers=[500,10]).to(device) \n",
        "model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW3ACDejoWNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mOTZLW_n-I3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 12\n",
        "running_loss_history = []\n",
        "running_correct_history = []\n",
        "val_running_loss_history = []\n",
        "val_running_correct_history = []\n",
        "\n",
        "print(\"len of training loader {}\".format(len(training_loader)))\n",
        "print(\"len of validation loader {}\".format(len(validation_loader)))\n",
        "for e in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0.0\n",
        "  validation_running_loss = 0.0\n",
        "  validation_running_corrects = 0.0\n",
        "  \n",
        "  for inputs, labels in training_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "    running_loss += loss.item()\n",
        "  with torch.no_grad():\n",
        "    for val_inputs, val_labels in validation_loader:\n",
        "      val_inputs = val_inputs.to(device)\n",
        "      val_labels = val_labels.to(device)\n",
        "      val_outputs = model(val_inputs)\n",
        "      val_loss = criterion(val_outputs, val_labels)\n",
        "      _, val_preds = torch.max(val_outputs, 1)\n",
        "      validation_running_corrects += torch.sum(val_preds == val_labels.data)\n",
        "      validation_running_loss += val_loss.item()\n",
        "\n",
        "  epoch_loss = running_loss/(len(training_loader))\n",
        "  acc = running_corrects/(len(training_loader))\n",
        "  val_epoch_loss = validation_running_loss/(len(validation_loader))\n",
        "  val_acc = validation_running_corrects/(len(validation_loader))\n",
        "\n",
        "  val_running_loss_history.append(val_epoch_loss)\n",
        "  running_loss_history.append(epoch_loss)\n",
        "  running_correct_history.append(acc)\n",
        "  val_running_correct_history.append(val_acc)\n",
        "\n",
        "  print(\"training loss : {:.4f} training accuracy : {:.2f}\".format(epoch_loss, acc))\n",
        "  print(\"Validation loss : {:.4f} Validation accuracy : {:.2f}\".format(val_epoch_loss, val_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-yu0nWh3WZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(validation_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "output = model(images)\n",
        "_, preds = torch.max(output, 1)\n",
        "\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "\n",
        "for idx in np.arange(20):\n",
        "  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "  plt.imshow(image_convert(images[idx]))\n",
        "  ax.set_title(\"{};[{}]\".format(str(preds[idx].item()), str(labels[idx].item())), color=(\"green\" if preds[idx].item() == labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT6jlgTEcB2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(running_loss_history, label=\"training loss\")\n",
        "plt.plot(val_running_loss_history, label=\"validation loss\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NEupmQbYzslu",
        "colab": {}
      },
      "source": [
        "plt.plot(running_correct_history, label=\"training accuracy\")\n",
        "plt.plot(val_running_correct_history, label=\"validation accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}